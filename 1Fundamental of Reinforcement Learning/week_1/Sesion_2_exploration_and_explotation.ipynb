{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration and Exploitation\n",
    "- Exploration improve knowledge for long-term benefits\n",
    "- Exploitation exploit current knowledge for short-term benefits, asigned probably wrong values to the other actions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture:\n",
    "- We discussed the tradeoff (compensation) betwen exploration and exploitation.\n",
    "- We introduced the epsilon-greedy winch is a simple method for balancing exploration and explotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metodo epsilon-greedy\n",
    "Podriamos optar por explotar la mayor parte del tiempo y explorar solo de vez en cuando. Para ello, podemos usar un método simple llamado epsilon-greedy. Con este método, la mayoría de las veces, elegimos la mejor acción conocida, pero de vez en cuando, elegimos al azar entre todas las acciones con probabilidades iguales, independientemente de la calidad de la acción. Es decir, con probabilidad 1- $$ \\epsilon $$, elegimos la mejor acción conocida y con probabilidad $$ \\epsilon $$, elegimos una acción al azar. A veces, $$ \\epsilon $$ se denomina tasa de exploración, ya que determina con qué frecuencia elegimos una acción al azar.\n",
    "$\\epsilon$  Epssilon make reference to the probability of exploration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon-Greedy on 10 armed tesbed from Sutton and Barto\n",
    "<img src=\"epsilon_greedy.png\" alt=\"Epsilon-Greedy on 10 armed tesbed from Sutton and Barto\"> \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando $$ \\epsilon = 0 $$, el agente no realiza ningun paso de exploración, solo pasos **greedy**, cuando epsilon= 1, la recompensa mejora con el tiempo, es decir explora el 1 % de la veces. Cuan do epsilon=0.1 se obtiene más recompensa que con los otros métodos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimistic Initial Values\n",
    "Summary:\n",
    "- Optimistic Initial Values encourage early exploration.\n",
    "- Describe limitations  of otptimistic initial values. \n",
    "\n",
    "\n",
    "### Limitations of optimistic initial values\n",
    "\n",
    "- Optimistic initial values only drive early exploration.\n",
    "- They are not well-situed for non-stationary problems.\n",
    "- We may not know what the optimistic initial values should be.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upper-Confidence Bound (UCB) Action Selection\n",
    "The selection of actions with an upper bound confidence make use the uncertainty on the estimations for encorage the exploration.\n",
    "\n",
    "Si la región es grande, no estamos seguros de que el valor de la acción A esté cerca del valor estimado. En UCB, seguimos el principio del\n",
    "optimismo ante la incertidumbre. Esto simplemente significa que si no estamos seguros de algo, debemos asumir con optimismo que es bueno. Por ejemplo, supongamos que tenemos estas tres acciones con incertidumbres asociadas, nuestro agente no tiene ni idea de cuál es la mejor.\n",
    "Por lo tanto, elige con optimismo la acción que tiene el límite superior más alto.\n",
    "Esto tiene sentido porque, o bien tiene el valor más alto y obtenemos una buena recompensa, o bien, al realizarla obtenemos información sobre una acción que conocemos menos.\n",
    "\n",
    "$$ A_t=argmax_a[Q_t(a)+c\\sqrt{\\frac{ln(t)}{N_t(a)}}] $$\n",
    "\n",
    "$$exploit= Q_t(a)$$\n",
    "\n",
    "$$explore= c\\sqrt{\\frac{ln(t)}{N_t(a)}}$$\n",
    "\n",
    "El parametro **c** definido por el usuario controla la cantidad de explotación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### contextual bandit tutorial http://hunch.net/~rwil \n",
    "### \" En una implementación de RL en simulador nostros tenemos el control, pero en una implementación del mundo real el control lo tiene el mundo real, el define el ritmo de trabajo.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
