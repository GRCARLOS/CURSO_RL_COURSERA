{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Process (MDP)\n",
    "\n",
    "En este curso, supndremos que el conjunto de estados,acciones y recompensas, son finitos.\n",
    "Dado que **p** es una distribución de probabilidad,debe ser no negativa y su suma sobre todos los estados y \n",
    "recompensas debe ser 1. \n",
    "\n",
    "**Observe que el estado y la recompensa futura, solo dependes del estado y de la acción\n",
    "actuales**. Esto se denomina **propiedad de Markov**.\n",
    "Significa que el estado actual es suficiente y que recordar estados anteriores, no mejorara las predicciones acerca del futuro. In summary:\n",
    "\n",
    "- MDPs porvee un marco de trabajo general para la toma de decisiones secuenciales.\n",
    "- La dinamica de MDP esta definida por una distribución de probabilidad.\n",
    "- El MDP framework puede utilizarse para formalizar una amplia variedad de problemas de toma de decisiones sequenciales. (Como el problema del robot limpiador de latas con 2 estados {batery low, batery high} y 3 acciones {search,wait, recharge,rescue}).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The goal of reinforcement learning\n",
    "\n",
    "El retorno **G** es la suma de las recompensas obtenidas de las interacciones del agente con el entorno.\n",
    "El objetivo del agente es maximizar el retorno esperado.\n",
    "En tareas episodicas, la interacción agente-entorno se divide en episodios.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Michel littma: The Reward Hypothesis\n",
    "intelligent behavior arises from the actions of an individual seeking to maximize its received reward \n",
    "signal in a complex and changing world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episodic tasks\n",
    "- La interacción se divede de forma natural en espisodios.\n",
    "- Cada episodio finaliza en un estado terminal.\n",
    "- Los episodios son independientes.\n",
    "\n",
    "$G_t \\doteq r_{t+1} + R_{t+2} + R_{t+3} + ...+ R_{T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuing tasks\n",
    "- En tareas continuas, la interacción agente-entorno  se da de forma indefinida.\n",
    "- El Discounting se utiliza para asegurar un retorno finito en tareas de caracter continuo, de \n",
    "lo contrario elretorno seria infinito.\n",
    "\n",
    "$G_t \\doteq R_{t+1} + \\gamma R_{t+2}  + \\gamma^{2} R_{t+3}  + \\gamma^{3} R_{t+4}...$\n",
    "\n",
    "$  = R_{t+1} + \\gamma (R_{t+2}  + \\gamma R_{t+3}  + \\gamma^{2} R_{t+4}...)$\n",
    "\n",
    "$ G_t = R_{t+1} + \\gamma G_{t+1}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ejemplos concretos de tarea episodica se mostro un problema parecido al gridworld, en el que un episodio termina cuando el agente toca recuadros de un color determiando. Como ejemplo de tarea continua se proporcione el ejemplo de servidores que estan continuamente liberandose y ocupandose, y el agente debe decidir cuando ocupar un servidor y cuando liberarlo, pero de forma indefinida."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
