{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifyng policies\n",
    "### Objetivs:\n",
    "- Reconocer que una politica es un distribución sobre acciones para cada posible estado.\n",
    "- Describir las similitudes y diferencias entre politicas deterministas y estocasticas.\n",
    "- Generar ejemplos de poiticas validas para un determinado MDP.\n",
    "Una politica $\\pi$ determinista, tiene lugar cuando una accion es seleccionada con probabilidad 1, y las demas acciones con probabilidad 0.\n",
    "Una politica $\\pi $ estocastica, tiene lugar cuando multiples accione pueden ser seleccionadas con una probabilidad diferente de 0. La suma de todas las probabilides de acción es igual a la unidad. La probabilidad de cada acción deber ser no negativa.\n",
    "\n",
    "- Muy importante, ** las politicas dependen solamente del estado actual, y no de otras cosas como tiempo y estados previos.m** Es mejor pensar en esto como un requisito del estado y no como una limitante del agente.\n",
    "\n",
    "Summary \n",
    "- Una politica mapea el estado actual a una distribución de probabilidad sobre acciones.\n",
    "- Las politicas solo puedender del estado actual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value functions\n",
    "### Objetivos:\n",
    "- Describir los roles de state-value y action-value functions en reinforcement learning.\n",
    "- Describir la relación entre value-functions y policies.\n",
    "- Crear ejemplo de value-functions validos para determinados MDPs.\n",
    "\n",
    "- The state value function $v_{\\pi}(s)$ de una politica $\\pi$ es la expectativa del retorno (suma de recompensas futuras), dada que el agente se encuentra en el estado $s$ y sigue la politica $\\pi$.\n",
    "\n",
    "$ v_{\\pi}(s)  \\doteq \\mathbb{E}_{\\pi} \\left[ G_{t} | S_{t}= s  \\right] $\n",
    "- The action value function describe  que pasa cuando el agente primero selecciona una acción en particular. The action value of state es el retorno esperado si el agente selcciona una acción A y luego sigue una politica $\\pi$.\n",
    "\n",
    "$ q_{\\pi}(s,a) \\doteq \\mathbb{E}_{\\pi} \\left[ G_{t} | S_{t}= s, A_{t}=a  \\right] $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sumary\n",
    "- The state value functions representan el retorno esperado de un estdo dado bajo un pollicy especifica.\n",
    "\n",
    "- The action value functions representan el retorno esperado de un estado depúes de tomar un acción especifica, y luego seguir una politica especifica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
